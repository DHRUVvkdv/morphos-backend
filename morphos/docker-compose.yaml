services:
  backend-service:
    build:
      context: ./services/backend-service
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./services/backend-service/src:/app
    environment:
      - DEBUG=true
      - ENVIRONMENT=development
      - CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080"]
      - INFERENCE_SERVICE_URL=http://inference-service:8000
    depends_on:
      - inference-service
    command: python main.py

  inference-service:
    build:
      context: ./services/inference-service
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
      - "8001:8001"  # Triton HTTP
      - "8002:8002"  # Triton gRPC
    volumes:
      - ./services/inference-service/src:/app
    environment:
      - DEBUG=true
      - ENVIRONMENT=development
      - TRITON_URL=http://localhost:8001
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]